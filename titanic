import time
import os
import pandas as pd
import numpy as np
from sklearn.model_selection import GridSearchCV, cross_val_score
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score
from sklearn.preprocessing import OneHotEncoder, StandardScaler
import matplotlib.pyplot as plt
from sklearn.impute import SimpleImputer

start_time = time.time()

########## SETUP ##########
###########################
# Make the data run the same every time it's run 
np.random.seed(42)

# Save fig
def save_fig(fig_id, tight_layout=True, fig_extension="png", resolution=300):
    path = os.path.join(fig_id + "." + fig_extension)
    print("Saving figure", fig_id)
    if tight_layout:
        plt.tight_layout()
    plt.savefig(path, format=fig_extension, dpi=resolution)

########## Load Data ##########
###############################
def load_train_data():
    return pd.read_csv('/Users/richard/Documents/Python/Machine-Learning/titanic/train.csv')
train_data = load_train_data()

def load_test_data():
    return pd.read_csv('/Users/richard/Documents/Python/Machine-Learning/titanic/test.csv')
test_data = load_test_data()

########## Evaluate Data ##########
###################################

print("""
Train Data Summary
""", train_data)

print("""
Test Data Summary
""", test_data.info())

#train_data.plot(kind="scatter", x="Age", y="Fare", alpha=0.1)
#save_fig("attribute_histogram_plots")
#plt.show()

########## Calibrate Train Data ##########
##########################################
# Drop irrelavant columns
prepared_train_slim = train_data.drop(['Name', 'Ticket', 'Cabin'], axis=1)

# Separate predictor and labels from prepared_train_data
train_data_predictors = prepared_train_slim.drop("Survived", axis=1)
train_data_labels = prepared_train_slim["Survived"].copy()

imputer = SimpleImputer(strategy="most_frequent")
imputer.fit(train_data_predictors)
x = imputer.transform(train_data_predictors)
train_data_predictors_transformed = pd.DataFrame(x, columns=train_data_predictors.columns, index=train_data_predictors.index)

# Convert alpha to numerical 
#prepared_train_data['Sex'].replace('male', 0, inplace=True)
#prepared_train_data['Sex'].replace('female', 1, inplace=True)
#prepared_train_data['Embarked'].replace('S', 1, inplace=True)
#prepared_train_data['Embarked'].replace('C', 2, inplace=True)
#prepared_train_data['Embarked'].replace('Q', 3, inplace=True)

# Calculate averages for missing data
#age_median = prepared_train_data["Age"].mean()
#embarked_median = prepared_train_data["Embarked"].median()
#fare_median = prepared_train_data["Fare"].median()

# Fill in the NULLs
#prepared_train_data["Age"].fillna(age_median, inplace=True)
#prepared_train_data["Embarked"].fillna(embarked_median, inplace=True)

# Use onehotencoder to organise converted data
encoder = OneHotEncoder(sparse=False)

train_data_predictors_encoded = pd.DataFrame(encoder.fit_transform(train_data_predictors_transformed[["Pclass", "Sex", "Fare", "Embarked"]]))
train_data_predictors_transformed_encoded = train_data_predictors_transformed.join(train_data_predictors_encoded)
train_data_predictors_transformed_encoded = train_data_predictors_transformed_encoded.drop(["Pclass", "Sex", "Embarked"], axis=1)

# Create new categories
prepared_train_data["family_size"] = prepared_train_data["SibSp"] + prepared_train_data["Parch"]
prepared_train_data = prepared_train_data.drop(["SibSp", "Parch"], axis=1)

# Scale the data
scaler = StandardScaler()
train_data_predictors_scaled = scaler.fit_transform(train_data_predictors.astype(np.float64))

########## Calibrate Test Data ##########
#########################################
# Drop irrelavant columns
prepared_test_data = test_data.drop(['Name', 'Ticket', 'Cabin'], axis=1)

# Convert alpha to numerical 
#prepared_test_data['Sex'].replace('male', 0, inplace=True)
#prepared_test_data['Sex'].replace('female', 1, inplace=True)
#prepared_test_data['Embarked'].replace('S', 1, inplace=True)
#prepared_test_data['Embarked'].replace('C', 2, inplace=True)
#prepared_test_data['Embarked'].replace('Q', 3, inplace=True)

# Fill in the NULLs 
#prepared_test_data["Age"].fillna(age_median, inplace=True)
#prepared_test_data["Embarked"].fillna(embarked_median, inplace=True)
#prepared_test_data["Fare"].fillna(fare_median, inplace=True)

prepared_test_data_transformed = imputer.transform(prepared_test_data)
prepared_test_data = pd.DataFrame(prepared_test_data_transformed, columns=prepared_test_data.columns, index=prepared_test_data.index)

# Use onehotencoder to organise converted data
test_cat_encoder = OneHotEncoder(sparse=False)
test_enc_df = pd.DataFrame(test_cat_encoder.fit_transform(prepared_test_data[["Pclass", "Sex", "Embarked"]]))
prepared_test_data = prepared_test_data.join(test_enc_df)
prepared_test_data = prepared_test_data.drop(["Pclass", "Sex", "Embarked"], axis=1)

# Create new categories
prepared_test_data["family_size"] = prepared_test_data["SibSp"] + prepared_test_data["Parch"]
prepared_test_data = prepared_test_data.drop(["SibSp", "Parch"], axis=1)

# Scale the data
prepared_test_data_scaled = scaler.fit_transform(prepared_test_data.astype(np.float64))

########## FINAL MODEL ##########
#################################

forest_clf = RandomForestClassifier(random_state=42)

# Fine Tune using GridSearch 
param_grid = [{'n_estimators': [10, 30, 60], 'max_features': [2, 4, 8, 14]},]
grid_search = GridSearchCV(forest_clf, param_grid, cv=3)
grid_search.fit(train_data_predictors_scaled, train_data_labels)

final_model = grid_search.best_estimator_

#############################################################
########## Use the final model to predict Survived ##########
#############################################################

survived_prediction = final_model.predict(prepared_test_data_scaled) # Perform the prediction


print("""
Grid Search - best parameters
""", grid_search.best_params_)

print("""
Cross Validation Score - Train Data
""", cross_val_score(forest_clf, train_data_predictors_scaled, train_data_labels, cv=3, scoring="accuracy"))

print("""
Cross Validation Score - Test Data
""", cross_val_score(forest_clf, prepared_test_data_scaled, survived_prediction, cv=3, scoring="accuracy"))

survived_prediction = pd.DataFrame(survived_prediction)
survived_prediction.to_csv('file.csv') 

print ("""
My program took""", time.time() - start_time, "to run")











