import time
import os
import pandas as pd
import numpy as np
from sklearn.model_selection import GridSearchCV, cross_val_score
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score
from sklearn.preprocessing import OneHotEncoder, StandardScaler
import matplotlib.pyplot as plt
from sklearn.impute import SimpleImputer

start_time = time.time()

########## SETUP ##########
###########################
# Make the data run the same every time it's run 
np.random.seed(42)

imputer = SimpleImputer(strategy="median")
encoder = OneHotEncoder(sparse=False)
scaler = StandardScaler()
forest_clf = RandomForestClassifier(random_state=42)

########## Load Data ##########
###############################
train_data = pd.read_csv('train.csv')
test_data = pd.read_csv('test.csv')

########## Evaluate Data ##########
###################################

# View a sample of the data
print("""
Train Data Sample
""", train_data.head())

# View a summary of the data
print("""
Train Data Summary
""", train_data.info())

# View the averages of the numerical data
print("""
Train Data Summary
""", train_data.describe())

########## Separate the Train Data ##########
#############################################


########## Clean the Train Data ##########
##########################################

### Passenger Id

### Survived
# Separate predictor and labels from prepared_train_data
train_data_predictors = train_data.drop("Survived", axis=1)
train_data_labels = train_data["Survived"]

### Pclass

### Name
for name_string in train_data_predictors['Name']:
    train_data_predictors['Title'] = train_data_predictors['Name'].str.extract('([A-Za-z]+)\.', expand=True)

print("""
Train data title value_count
""", train_data_predictors["Title"].value_counts())

# Group together like titles
title_mapping = {'Lady':'Rare', 'Countess':'Rare', 'Capt':'Rare',
                'Col':'Rare', 'Don':'Rare', 'Dr':'Rare', 'Major':'Rare', 
                'Rev':'Rare', 'Sir':'Rare', 'Jonkheer':'Rare', 'Dona':'Rare', 
                'Countess':'Rare', 'Mme':'Mrs', 'Mlle':'Mrs'}

train_data_predictors.replace({'Title':title_mapping}, inplace=True)

train_data_predictors["Title"] = train_data_predictors["Title"].map({"Master":0, "Miss":3, "Mr" : 2 ,"Mrs":3, "Rare":4,})

train_data_predictors = train_data_predictors.drop("Name", axis=1)

print("""
Train data title value_count
""", train_data_predictors["Title"].value_counts())

### Sex
train_data_predictors['Sex'].replace('male', 0, inplace=True)
train_data_predictors['Sex'].replace('female', 1, inplace=True)

### Age

### SibSp

### Parch

### Ticket
train_data_predictors = train_data_predictors.drop("Ticket", axis=1)

### Fare

### Cabin
train_data_predictors = train_data_predictors.drop("Cabin", axis=1)

### Embarked
train_data_predictors['Embarked'].replace('S', 0, inplace=True)
train_data_predictors['Embarked'].replace('C', 1, inplace=True)
train_data_predictors['Embarked'].replace('Q', 2, inplace=True)

# View a sample of the data
print("""
Ammended Train Data Sample
""", train_data_predictors.head())

# Use SimpleImputer to fill missing data
imputer.fit(train_data_predictors)
x = imputer.transform(train_data_predictors)
train_data_predictors_transformed = pd.DataFrame(x, columns=train_data_predictors.columns, index=train_data_predictors.index)

# Use OneHotEncoder to encode alpha data, and numerical with more than three possibilities
train_data_predictors_encoded = pd.DataFrame(encoder.fit_transform(train_data_predictors_transformed[["Pclass", "Embarked", "Title"]]))
train_data_predictors_transformed_encoded = train_data_predictors_transformed.join(train_data_predictors_encoded)

# Create new categories
train_data_predictors_transformed_encoded["family_size"] = train_data_predictors_transformed_encoded["SibSp"] + train_data_predictors_transformed_encoded["Parch"]

# Drop irrelevant categories
train_data_predictors_transformed_encoded = train_data_predictors_transformed_encoded.drop(["Pclass", "Embarked", "SibSp", "Parch", "Title"], axis=1)

# Scale the data
prepared_train_data_predictors = scaler.fit_transform(train_data_predictors_transformed_encoded.astype(np.float64))

########## Calibrate Test Data ##########
##########################################

### Passenger Id

### Pclass

### Name
for name_string in test_data['Name']:
    test_data['Title'] = test_data['Name'].str.extract('([A-Za-z]+)\.', expand=True)

print("""
Train data title value_count
""", test_data["Title"].value_counts())

# Group together like titles
title_mapping = {'Lady':'Rare', 'Countess':'Rare', 'Capt':'Rare',
                'Col':'Rare', 'Don':'Rare', 'Dr':'Rare', 'Major':'Rare', 
                'Rev':'Rare', 'Sir':'Rare', 'Jonkheer':'Rare', 'Dona':'Rare', 
                'Countess':'Rare', 'Mme':'Mrs', 'Mlle':'Mrs'}

test_data.replace({'Title':title_mapping}, inplace=True)

test_data["Title"] = test_data["Title"].map({"Master":0, "Miss":3, "Mr" : 2 ,"Mrs":3, "Rare":4,})

test_data = test_data.drop("Name", axis=1)

print("""
Train data title value_count
""", test_data["Title"].value_counts())

### Sex
test_data['Sex'].replace('male', 0, inplace=True)
test_data['Sex'].replace('female', 1, inplace=True)

### Age

### SibSp

### Parch

### Ticket
test_data = test_data.drop("Ticket", axis=1)

### Fare

### Cabin
test_data = test_data.drop("Cabin", axis=1)

### Embarked
test_data['Embarked'].replace('S', 0, inplace=True)
test_data['Embarked'].replace('C', 1, inplace=True)
test_data['Embarked'].replace('Q', 2, inplace=True)

# View a sample of the data
print("""
Ammended Train Data Sample
""", test_data.head())

# Use SimpleImputer to fill missing data
imputer.fit(test_data)
x = imputer.transform(test_data)
test_data_transformed = pd.DataFrame(x, columns=test_data.columns, index=test_data.index)

# Use OneHotEncoder to encode alpha data, and numerical with more than three possibilities
test_data_encoded = pd.DataFrame(encoder.fit_transform(test_data_transformed[["Pclass", "Embarked", "Title"]]))
test_data_transformed_encoded = test_data_transformed.join(test_data_encoded)

# Create new categories
test_data_transformed_encoded["family_size"] = test_data_transformed_encoded["SibSp"] + test_data_transformed_encoded["Parch"]

# Drop irrelevant categories
test_data_transformed_encoded = test_data_transformed_encoded.drop(["Pclass", "Embarked", "SibSp", "Parch", "Title"], axis=1)

# Scale the data
prepared_test_data = scaler.fit_transform(test_data_transformed_encoded.astype(np.float64))

########## FINAL MODEL ##########
#################################
# Fine Tune using GridSearch 
param_grid =    [{'n_estimators': [30, 60, 100, 200, 300, 400], 'max_features': [0, 1, 2, 4, 8, 10, 14]},
                {'bootstrap': [False], 'n_estimators': [3, 10], 'max_features': [2, 3, 4]},]
grid_search = GridSearchCV(forest_clf, param_grid, cv=4)
grid_search.fit(prepared_train_data_predictors, train_data_labels)

final_model = grid_search.best_estimator_

########## Display accuracy scores ##########
#############################################
print("""
Grid Search - best parameters
""", grid_search.best_params_)

forest_scores = cross_val_score(final_model, prepared_train_data_predictors, train_data_labels, cv=4, scoring="accuracy")
print("""
Mean Cross Validation Score - Train Data
""", forest_scores.mean())

########## Use the final model to predict Survived ##########
#############################################################

survived_prediction = final_model.predict(prepared_test_data) # Perform the prediction

########## Save results ##########
##################################
survived_prediction = pd.DataFrame(survived_prediction)
survived_prediction.to_csv('file.csv') 

print ("""
My program took""", time.time() - start_time, "to run")

